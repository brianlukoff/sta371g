\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{1, 0.894, 0.769}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.824,0.412,0.118}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{1,0.894,0.71}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.824,0.706,0.549}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.941,0.902,0.549}{#1}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.804,0.776,0.451}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.78,0.941,0.545}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{1,0.78,0.769}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{../371g-slides}
\title{Multiple regression}
\subtitle{Lecture 15}
\author{STA 371G}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
  
  

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Why do some colleges have higher graduation rates than others?}
      \begin{itemize}[<+->]
        \item What factors do you think impact the graduation rate of a college?
        \item It seems like there is no \emph{one} factor that dominates---it is probably true that to make a good prediction we need to put a lot of variables together, so simple regression is likely not sufficient.
        \item \alert{Multiple regression} allows us to build on simple regression by predicting one $Y$ variable using multiple $X$ variables.
      \end{itemize}
    \end{frame}

    \begin{frame}{The colleges data set}
      Today's data set is a sample of 1302 colleges with various factors about the colleges, including SAT scores, student/faculty ratios, tuition rates, acceptance rates, etc.
    \end{frame}

    \begin{frame}[fragile]{A quick data clean}
      Many colleges have no SAT scores reported, so let's ignore those colleges (to enable a fair comparison) and also remove colleges with an obviously incorrect graduation rate of $>100\%$:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{my.sample} \hlkwb{<-} \hlkwd{subset}\hlstd{(colleges,}
  \hlopt{!}\hlkwd{is.na}\hlstd{(Average.combined.SAT)} \hlopt{&} \hlstd{Graduation.rate} \hlopt{<=} \hlnum{100}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]

      \begin{center}
        SAT scores and (in-state) tuition were the two best single predictors, with $R^2$ values of 0.353 and 0.325, respectively. Can we combine these together and get an $R^2$ that is better than either predictor would produce on its own?
      \end{center}
    \end{frame}

    \begin{frame}{Using multiple predictors to predict graduation rate}
      The simple regression models were:
      \[
        Y_i = \beta_0 + \beta_1 (\text{SAT}) + \epsilon_i
      \]
      and
      \[
        Y_i = \beta_0 + \beta_1 (\text{tuition}) + \epsilon_i.
      \]
      The multiple regression model is
      \[
        Y_i = \beta_0 + \beta_1 (\text{SAT}) + \beta_2 (\text{tuition}) + \epsilon_i.
      \]
    \end{frame}

    \begin{frame}[fragile]
      \fontsize{8}{8}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(Graduation.rate} \hlopt{~} \hlstd{Average.combined.SAT} \hlopt{+} \hlstd{In.state.tuition,} \hlkwc{data}\hlstd{=my.sample)}
\hlkwd{summary}\hlstd{(model)}
\end{alltt}
\begin{verbatim}

Call:
lm(formula = Graduation.rate ~ Average.combined.SAT + In.state.tuition, 
    data = my.sample)

Residuals:
   Min     1Q Median     3Q    Max 
-45.53  -9.18   0.05   8.70  43.66 

Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
(Intercept)          -8.324646   4.370828    -1.9    0.057 .  
Average.combined.SAT  0.061122   0.004888    12.5   <2e-16 ***
In.state.tuition      0.001249   0.000111    11.2   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 13.7 on 709 degrees of freedom
  (19 observations deleted due to missingness)
Multiple R-squared:  0.447,	Adjusted R-squared:  0.445 
F-statistic:  286 on 2 and 709 DF,  p-value: <2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}
      The multiple regression prediction equation is:

      \[
        \widehat{\text{Graduation rate}} =
          -8.3246 +
          0.0611(\text{SAT})
          + 0.0012(\text{tuition})
      \]
      \pause
      We can use this to make predictions like we would for a simple regression!
    \end{frame}

    \begin{frame}[fragile]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-6-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-7-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{Interpreting the coefficients: intercept}
      Let's interpret the intercept coefficient of $-8.3246$:
      \begin{itemize}[<+->]
        \item The predicted graduation rate when the average SAT score is 0 and the in-state tuition is \$0 is $-8.3246$.
        \item This is not a meaningful number on its own in this case, since there will never be a school with those particular predictor values! (The intercept might be interpretable for other models.)
      \end{itemize}
    \end{frame}

    \begin{frame}{Interpreting the coefficients: SAT}
      Let's interpret the SAT coefficient of 0.0611:
      \begin{itemize}[<+->]
        \item \alert{Holding tuition constant}, each additional SAT score point  increases our predicted graduation rate by 0.0611 percentage points.
        \item \alert{Among colleges that have the same tuition}, an increase in SAT of 1 point would result in a predicted graduation rate that is 0.0611 percentage points higher.
        \item \alert{If we compared two colleges that have the same tuition but differ in average SAT scores by 1 point}, the college with the higher SAT score would be predicted to have a graduation rate that is 0.0611 percentage points higher.
      \end{itemize}
    \end{frame}

    \begin{frame}{Interpreting the coefficients: tuition}
      Let's interpret the tuition coefficient of 0.0012:
      \begin{itemize}[<+->]
        \item \alert{Holding SAT constant}, each additional dollar of in-state tuition increases our predicted graduation rate by 0.0012 percentage points.
        \item \alert{Among colleges that have the same average SAT scores}, an increase in tuition of \$1 would result in a predicted graduation rate that is 0.0012 percentage points higher.
        \item \alert{If we compared two colleges that have the same average SAT scores but differ in their tuition by \$1}, the college with the higher tuition would be predicted to have a graduation rate that is 0.0012 percentage points higher.
      \end{itemize}
    \end{frame}

    \begin{frame}{What's the difference?!}
      \begin{itemize}[<+->]
        \item ``The predicted effect of a 1-point increase in SAT score'' and ``the predicted effect of a 1-point increase in SAT score, holding tuition constant'' really are \alert{two different things}.
        \item The relationship between $X_1$ and $Y$ may change when we \alert{control for} (i.e., add to the model) another predictor $X_2$.
      \end{itemize}
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      We need (the same!) four things to be true for statistical inference (i.e., hypothesis tests, $p$-values, confidence intervals) to work for multiple regression:
      \pause
      \begin{enumerate}
        \item The errors are independent.
        \item $Y$ is a linear function of the $X$'s (except for the errors).
        \item The errors are normally distributed.
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity'').
      \end{enumerate}
    \end{frame}

    \begin{frame}{Assumption 1: Independence of errors}
      Independence means that knowing the error (over-/under-prediction by the regression line) for one case doesn't tell you anything about the error for another case.

      \bigskip\pause

      Since each college is completely separate, there is no reason to think the errors are not independent.
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      \begin{enumerate}
        \item The errors are independent. \greencheckmark
        \item $Y$ is a linear function of the $X$'s (except for the errors).
        \item The errors are normally distributed.
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity'').
      \end{enumerate}
    \end{frame}

    \begin{frame}[fragile]{Assumption 2: Linearity}
      Look at the residual plot:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(}\hlkwd{predict}\hlstd{(model),} \hlkwd{residuals}\hlstd{(model),} \hlkwc{col}\hlstd{=}\hlstr{"green"}\hlstd{,}
  \hlkwc{xlab}\hlstd{=}\hlstr{"Predicted values"}\hlstd{,} \hlkwc{ylab}\hlstd{=}\hlstr{"Residuals"}\hlstd{,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwc{h}\hlstd{=}\hlnum{0}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-9-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      \begin{enumerate}
        \item The errors are independent. \greencheckmark
        \item $Y$ is a linear function of the $X$'s (except for the errors). \greencheckmark
        \item The errors are normally distributed.
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity'').
      \end{enumerate}
    \end{frame}

    \begin{frame}[fragile]{Assumption 3: Normality of residuals}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qqnorm}\hlstd{(}\hlkwd{residuals}\hlstd{(model))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-10-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      \begin{enumerate}
        \item The errors are independent. \greencheckmark
        \item $Y$ is a linear function of the $X$'s (except for the errors). \greencheckmark
        \item The errors are normally distributed. \greencheckmark
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity'').
      \end{enumerate}
    \end{frame}

    \begin{frame}[fragile]{Assumption 4: Homoscedasticity}
      Look at the residual plot:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(}\hlkwd{predict}\hlstd{(model),} \hlkwd{residuals}\hlstd{(model),} \hlkwc{col}\hlstd{=}\hlstr{"green"}\hlstd{,}
  \hlkwc{xlab}\hlstd{=}\hlstr{"Predicted values"}\hlstd{,} \hlkwc{ylab}\hlstd{=}\hlstr{"Residuals"}\hlstd{,} \hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-11-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{Multiple regression assumptions}
      \begin{enumerate}
        \item The errors are independent. \greencheckmark
        \item $Y$ is a linear function of the $X$'s (except for the errors). \greencheckmark
        \item The errors are normally distributed. \greencheckmark
        \item The variance of $Y$ is the same for any value of $X$ (``homoscedasticity''). \shrug[red]
      \end{enumerate}
    \end{frame}

    \begin{frame}
      \begin{center}
        Since one of the assumptions is not completely satisfied, we'll proceed with caution---i.e., take the $p$-values and confidence intervals with a grain of salt.  (We could try and fix the problem with a transformation, or by building different models for different subsets of the data, but let's just live with it for now.)
      \end{center}
    \end{frame}

    \begin{frame}{The overall null hypothesis for a regression model}
      The following are equivalent ways to express the overall null hypothesis with $k$ predictor variables:
      \begin{itemize}[<+->]
        \item $R^2=0$ (in the population)
        \item $\text{cor}(\hat Y,Y)=0$ (in the population)
        \item $\beta_1=\beta_2=\cdots=\beta_k=0$ (i.e., all coefficients are 0 except the intercept)
        \item The model has no predictive power
        \item Predictions from this model are no better than predicting $\overline Y$ for every case
      \end{itemize}
    \end{frame}

    \begin{frame}
      We should always test the overall null hypothesis for a model first. \alert{If we can't reject the overall null hypothesis, there's no reason to interpret the model further.}

      \bigskip\pause

      In this model, the overall $p$-value is very small, so we reject the overall null hypothesis and conclude that yes, we have statistical significance and that this model does have some predictive power.
    \end{frame}

    \begin{frame}{Statistical vs practical significance}
      \begin{itemize}
        \item As in simple regression, once we determine that there is statistical significance, we want to then assess whether there is also practical significance.
        \item For the test of the overall null hypothesis, we look to the value of $R^2$ in the sample to assess practical significance.
      \end{itemize}
    \end{frame}

    \begin{frame}{Testing individual coefficients}
      \begin{itemize}[<+->]
        \item Next, we want to test $H_0 : \beta_i=0$ for each of the predictors $X_i$, for each $i=1,2,\ldots,k$.
        \item This is equivalent to the null hypothesis that $X_i$ has no correlation with $Y$ once the other predictors are held constant.
        \item The test statistic for testing the null hypothesis $\beta_i = S$ follows a $t$-distribution with $n-k-1$ degrees of freedom: \[ t = \frac{\beta_i - S}{\text{SE}(\beta_i)} \]
        \item The regression output calculates the $p$-value for us for testing the null hypotheses $\beta_i = 0$.
        \item If we reject this null hypothesis for a coefficient, we say that $X_i$  is a (statistically) significant predictor of $Y$ in the model.
      \end{itemize}
    \end{frame}

    \begin{frame}{Testing individual coefficients}
      If a predictor is not statistically significant, we should:
      \begin{enumerate}[<+->]
        \item Interpret it as if it were zero.
        \item Remove it from the model (unless there are other reasons to keep it), as it does not contribute to predicting $Y$ above and beyond the other predictors.
      \end{enumerate}
    \end{frame}

    % \begin{frame}{Comparing two models}
    %   <<echo=F, results="asis">>=
    %   model1 <- lm(Graduation.rate ~ Acceptance.rate, data=my.sample)
    %   model2 <- lm(Graduation.rate ~ Acceptance.rate + Average.combined.SAT, data=my.sample)
    %   stargazer(model1, model2,
    %     title="Models for predicting graduation rate",
    %     dep.var.caption="",
    %     report="vc*",
    %     dep.var.labels.include=F,
    %     column.labels=c("Model 1", "Model 2"),
    %     align=T,
    %     single.row=T,
    %     omit.stat=c("ser", "f", "n"),
    %     omit.table.layout=c("m")
    %   )
    %   @
    % \end{frame}

    \begin{frame}{Residual standard error}
      \begin{itemize}[<+->]
        \item Like with simple regression, the \alert{residual standard error} $s_e$ is approximately equal to the standard deviation of the residuals.
        \item Since one of the assumptions of regression is that the residuals are approximately normal, we can conclude that approximately 95\% of the residuals will be less than $\pm 2s_e$.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Confidence intervals for coefficents}
      Confidence intervals for the individual coefficients are found the same way as in simple regression, and interpreted the same way:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{confint}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
                         2.5 %   97.5 %
(Intercept)          -16.90596 0.256669
Average.combined.SAT   0.05153 0.070718
In.state.tuition       0.00103 0.001467
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Confidence intervals for predictions}
      \fontsm
      We can also put confidence intervals on our predictions for $Y$.

      \bigskip\pause

      A 95\% CI for the graduation rate at the University of California, Merced, which is not in the data set and has an average SAT score of 1100 and in-state tuition of \$11,502:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{predict}\hlstd{(model,} \hlkwd{list}\hlstd{(}\hlkwc{Average.combined.SAT}\hlstd{=}\hlnum{1100}\hlstd{,}
                    \hlkwc{In.state.tuition}\hlstd{=}\hlnum{11502}\hlstd{),}
               \hlkwc{interval}\hlstd{=}\hlstr{"prediction"}\hlstd{)}
\end{alltt}
\begin{verbatim}
    fit   lwr   upr
1 73.27 46.24 100.3
\end{verbatim}
\end{kframe}
\end{knitrout}
      

      \pause

      Our best guess for UC Merced is 73.27\%, with a 95\% CI of (46.24\%, 100.3\%). \pause (It turns out that the actual graduation rate at UC Merced is 64\%.)
    \end{frame}

    \begin{frame}[fragile]{Confidence intervals for predictions}
      \fontsm
      A 95\% CI for average graduation rate among all colleges with an average SAT score of 1100 and in-state tuition of \$11,502:
      
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{predict}\hlstd{(model,} \hlkwd{list}\hlstd{(}\hlkwc{Average.combined.SAT}\hlstd{=}\hlnum{1100}\hlstd{,}
                    \hlkwc{In.state.tuition}\hlstd{=}\hlnum{11502}\hlstd{),}
               \hlkwc{interval}\hlstd{=}\hlstr{"confidence"}\hlstd{)}
\end{alltt}
\begin{verbatim}
      fit     lwr     upr
1 73.2715 71.8091 74.7338
\end{verbatim}
\end{kframe}
\end{knitrout}

      \bigskip\pause

      As with simple regression, our point estimate is the same, but the confidence interval is much narrower, because it's easier to estimate a mean than a prediction for a single new case.
    \end{frame}


  \end{darkframes}

\end{document}
