\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{1, 0.894, 0.769}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.824,0.412,0.118}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{1,0.894,0.71}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.824,0.706,0.549}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.941,0.902,0.549}{#1}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.804,0.776,0.451}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.78,0.941,0.545}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{1,0.78,0.769}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{../371g-slides}
\title{What can go wrong, and how to fix it 1}
\subtitle{Lecture 13}
\author{STA 371G}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
  
  
  

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}
      \fullpagepicture{debbie-downer}
    \end{frame}

    \section{Extrapolation}

    \begin{frame}{Extrapolation}
      \begin{itemize}
        \item \alert{Extrapolation} means using the regression model to make a prediction outside of the range of the data
        \item The further we are outside of our ``comfort zone'' of the data in our sample, the less certain we should be of our prediction
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]
      The confidence interval for mean response gets wider as $X$ gets farther from $\overline X$:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-3-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}
      \begin{itemize}[<+->]
        \item There's nothing wrong with extrapolating a little bit beyond the data
        \item But if we are \emph{very} far outside of the range of our data, prediction intervals and confidence intervals for the mean response will tend to \emph{underestimate} the amount of uncertainty in our estimate
        \item For example, we wouldn't want to use this model to estimate the number of drinks consumed for someone that starts drinking at age 40!
      \end{itemize}
    \end{frame}

    \section{Influential observations}
    
    \begin{frame}{What a single case can do}
      \begin{center}
        Even a single case can wreak havoc on the regression line.
        Let's add one outlier, at $X=5$, and see what happens with different
        $Y$ values.
      \end{center}
    \end{frame}

    \begin{frame}{What a single case can do}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-4-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{What a single case can do}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-5-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{What a single case can do}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-6-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{What a single case can do}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-7-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}
      \fullpagepicture{blackmail}
    \end{frame}

    \begin{frame}{Regression is like blackmail}
      Blackmail:
      \begin{itemize}
        \item Compromising information gives a blackmailer \alert{leverage}---the \emph{potential} to have a big impact
        \item Once the blackmailer uses the information, that gives them \alert{influence}
      \end{itemize}
      \pause

      Regression:
      \begin{itemize}
        \item When a point has a very unusual $X$ value (i.e., far from $\overline X$), it has \alert{leverage}---the \emph{potential} to have a big impact on the regression line
        \item When that point \emph{also} has a $Y$ value that is out of line with the general trend, it will pull the regression line towards it---giving it \alert{influence}
      \end{itemize}
    \end{frame}

    \begin{frame}{How do I know what points are influential?}
      \begin{itemize}[<+->]
        \item \alert{Cook's distance} can be a useful metric for finding influential points
        \item Larger values of Cook's distance indicate more influential points, but there is no firm cutoff
        \item Use Cook's distance to help you find points that might be influential, and then run the regression both with and without the point to judge for yourself
      \end{itemize}
    \end{frame}

    \begin{frame}{Using the Cook's distance plot in R}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(model,} \hlkwc{which}\hlstd{=}\hlnum{5}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-8-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{What can we do about influential observations?}
      \begin{itemize}[<+->]
        \item If removing the influential observation doesn't make a substantial difference in your analysis, don't worry about it
        \item If it does:
          \begin{itemize}
            \item Consider whether it could be a mistake (happens more than you might think!); if it is, correct the error!
            \item If not, hold out the influential observation(s) and report on them separately
            \item Do not just throw out and ignore influential observations!
          \end{itemize}
      \end{itemize}
    \end{frame}

    \section{Autocorrelation}

    \begin{frame}{Residuals}
      Recall that the \alert{residual} for the $i$th case in the data is $Y_i - \hat Y_i$.

      \begin{itemize}
        \item When the residual is \emph{positive}, the actual $Y$-value is \emph{higher} than our predicted $Y$-value.
        \item When the residual is \emph{negative}, the actual $Y$-value is \emph{lower} than our predicted $Y$-value.
      \end{itemize}

      \bigskip

      Looking at residuals can tell us a lot about how well a model is working, and give us ideas for how to improve it.
    \end{frame}

    \begin{frame}{What is autocorrelation?}
      \begin{itemize}[<+->]
        \item The first assumption of regression is that the errors are independent.
        \item In many time series data sets, this isn't the case---what happens in one time period is often correlated with those time periods right before or after.
        \item \alert{Autocorrelation} is when we can consistently predict the value of a variable at a particular time based on other times.
      \end{itemize}
    \end{frame}

    \begin{frame}{How can autocorrelation be detected?}
      \begin{itemize}
        \item Sometimes, it's clear that there is likely to be autocorrelation.
        \item Any time that the value of a time series builds on the previous stage (e.g., daily stock price, annual revenue) autocorrelation is a likely danger.
        \item But it's worth checking for autocorrelation in any time series!
      \end{itemize}
    \end{frame}

    \begin{frame}{What about when it's not obvious?}
      The \alert{Durbin-Watson test} lets us test the null hypothesis that the errors in a regression come from a population where successive errors are uncorrelated.

      \bigskip\pause

      If we \textbf{reject} the null hypothesis, then there is evidence of autocorrelation, and the independence assumption is violated.
    \end{frame}

    \begin{frame}{Example}
      Consider the stock market data, when we regressed a company's weekly return on the weekly return of a market index (Wilshire 5000). Is autocorrelation present?

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-9-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{Example}
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(lmtest)}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(AMZN} \hlopt{~} \hlstd{W5000,} \hlkwc{data}\hlstd{=stock.market)}
\hlkwd{dwtest}\hlstd{(model)}
\end{alltt}
\begin{verbatim}

	Durbin-Watson test

data:  model
DW = 1.9759, p-value = 0.4249
alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}
\end{kframe}
\end{knitrout}
      Here, $p=0.42>0.05$, so we fail to reject the null hypothesis: there is no evidence of (first-order) autocorrelation. \pause (Surprised?)
    \end{frame}

    \begin{frame}{First-order?!}
      \begin{itemize}
        \item Durbin-Watson only lets us test for \alert{first-order} autocorrelation; that is, correlation between the error at time $t$ (today) and the error at time $t-1$ (yesterday, last month, last year, etc).
        \item Sometimes the error at time $t$ is correlated not with time $t-1$ but time $t-2$ or $t-3$, etc.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Example}
      Let's look at Apple's quarterly revenue since 2006:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-11-1} 

\end{knitrout}

      What do you think the peaks correspond to?
    \end{frame}

    \begin{frame}{The autocorrelation function}
      Another approach to suss out autocorrelation is to calculate all possible correlations with the time series and itself, ``lagged'' back by different time steps:

      \bigskip

      \begin{center}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\begin{tabular}{lrrrr}
\toprule
  & Apple revenue (\$B) & Lag 1 & Lag 2 & Lag 3\\
\midrule
3 & 4.837 & NA & NA & NA\\
4 & 7.115 & 4.837 & NA & NA\\
1 & 5.264 & 7.115 & 4.837 & NA\\
2 & 5.410 & 5.264 & 7.115 & 4.837\\
3 & 6.789 & 5.410 & 5.264 & 7.115\\
\addlinespace
4 & 9.608 & 6.789 & 5.410 & 5.264\\
\bottomrule
\end{tabular}


\end{knitrout}
      \end{center}
    \end{frame}

    \begin{frame}[fragile]{The autocorrelation function}
      The autocorrelations are highest for lag 1 and lag 4 (i.e., one quarter and one year ago):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{acf}\hlstd{(apple}\hlopt{$}\hlstd{Revenue.Billions)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-13-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{The autocorrelation function}
      The autocorrelations are highest for lag 1 and lag 4 (i.e., one quarter and one year ago):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{acf}\hlstd{(apple}\hlopt{$}\hlstd{Revenue.Billions)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-14-1} 

\end{knitrout}
    \end{frame}


    \begin{frame}[fragile]{The autocorrelation function, applied to residuals}
      \fontsm
      What we really need to test is the autocorrelation of the \emph{residuals}, not of the revenue itself. Durbin-Watson suggests there is no first-order autocorrelation:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(Revenue.Billions} \hlopt{~} \hlstd{Time,} \hlkwc{data}\hlstd{=apple)}
\hlkwd{dwtest}\hlstd{(model)}
\end{alltt}
\begin{verbatim}

	Durbin-Watson test

data:  model
DW = 1.8109, p-value = 0.1938
alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]

      But there is clearly second- and fourth-order autocorrelation!

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(Revenue.Billions} \hlopt{~} \hlstd{Time,} \hlkwc{data}\hlstd{=apple)}
\hlkwd{acf}\hlstd{(}\hlkwd{residuals}\hlstd{(model))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-16-1} 

\end{knitrout}

      How do we interpret these--what accounts for this pattern?
    \end{frame}

    \begin{frame}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-17-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{How to handle autocorrelation}
      \begin{itemize}[<+->]
        \item When time-series data is used, it's important to check for autocorrelation.
        \item If present, the independence assumption is violated, and we shouldn't trust the $p$-values and confidence intervals that come out of the regression.
        \item There may be a way to transform the data to remove the autocorrelation: for example, stock prices have strong autocorrelation, but the percentage changes from day to day (or week to week, etc.) do not.
      \end{itemize}
    \end{frame}

    \begin{frame}{A final note on regression assumptions}
      \begin{itemize}[<+->]
        \item The purpose of most regression assumptions is ensuring that the $p$-values and confidence intervals are accurate.
        \item But nothing prevents you from building a regression even when the assumptions are violated!
        \item Unless the linearity assumption is violated, the regression equation may still be useful for making predictions.
      \end{itemize}
    \end{frame}


    
 
  \end{darkframes}

\end{document}
