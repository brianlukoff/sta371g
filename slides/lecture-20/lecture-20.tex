\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{1, 0.894, 0.769}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.824,0.412,0.118}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{1,0.894,0.71}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.824,0.706,0.549}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{1,0.894,0.769}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.941,0.902,0.549}{#1}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.804,0.776,0.451}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.78,0.941,0.545}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{1,0.78,0.769}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{preview}
\usepackage{../371g-slides}
\title{Logistic Regression 2}
\subtitle{Lecture 20}
\author{STA 371G}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
  
  

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{Last time}

      \begin{itemize}
        \item The OkCupid data set contains information about 59946 profiles from users of the OkCupid online dating service.
        \item We predicted sex (as a binary categorical variable) from height using logistic regression, and came up with the prediction equation:
        \[
          \log\text{odds} = \log\left(\frac{P(\text{male})}{1-P(\text{male})}\right) = -44.45 + 0.66\cdot\text{height}.
        \]
        or, solving for $P(\text{male})$,
        \[
          \widehat{P(\text{male})} = \frac{e^{-44.45 + 0.66\cdot\text{height}}}{1 + e^{-44.45 + 0.66\cdot\text{height}}}
        \]
      \end{itemize}
    \end{frame}

    \section{Evaluating the model}

    \begin{frame}{How good is our model?}
      \begin{itemize}[<+->]
        \item Unfortunately, the typical $R^2$ metric isn't available for logistic regression.
        \item However, there are many ``pseudo-$R^2$'' metrics that indicate model fit.
        \item But: most of these pseudo-$R^2$ metrics are difficult to interpret, so we'll focus on something simpler to interpret and communicate.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{How many cases did we accurately predict?}
      We could use our model to make a prediction of sex based on the probability. Suppose we say that our prediction is:
      \[
        \text{Prediction} = \begin{cases}
          \text{male}, & \text{if $\widehat{P(\text{male})} \geq 0.5$}, \\
          \text{female}, & \text{if $\widehat{P(\text{male})} < 0.5$}. \\
        \end{cases}
      \]

      \pause
      Now we can compute the fraction of people whose sex we correctly predicted:

      \fontsize{9}{9}\selectfont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{predicted.male} \hlkwb{<-} \hlstd{(}\hlkwd{predict}\hlstd{(model,} \hlkwc{type}\hlstd{=}\hlstr{"response"}\hlstd{)} \hlopt{>=} \hlnum{0.5}\hlstd{)}
\hlstd{actual.male} \hlkwb{<-} \hlstd{(my.profiles}\hlopt{$}\hlstd{male} \hlopt{==} \hlnum{1}\hlstd{)}
\hlkwd{sum}\hlstd{(predicted.male} \hlopt{==} \hlstd{actual.male)} \hlopt{/} \hlkwd{nrow}\hlstd{(my.profiles)}
\end{alltt}
\begin{verbatim}
[1] 0.83
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]{How many cases did we accurately predict?}
      83\% sounds pretty good---what should we compare it against?

      \bigskip
      \pause

      We should compare 83\% against what we would have gotten if we just predicted the most common outcome (male) for everyone, without using any other information:
      \pause

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{sum}\hlstd{(actual.male)} \hlopt{/} \hlkwd{nrow}\hlstd{(my.profiles)}
\end{alltt}
\begin{verbatim}
[1] 0.6
\end{verbatim}
\end{kframe}
\end{knitrout}

      \pause

      In other words, our model provided a ``lift'' in accuracy from 60\% to 83\%.
    \end{frame}

    \begin{frame}{The confusion matrix}
      Sometimes it is useful to understand what kinds of errors our model is making:
      \begin{itemize}
        \item \alert{True positives}: predicting male for someone that is male
        \item \alert{True negatives}: predicting female for someone that is female
        \item \alert{False positives}: predicting male for someone that is female
        \item \alert{False negatives}: predicting female for someone that is male
      \end{itemize}
      (If we had designated female as 1 and male as 0, these would have switched!)
    \end{frame}

    \begin{frame}[fragile]{The confusion matrix}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{table}\hlstd{(predicted.male, actual.male)}
\end{alltt}
\begin{verbatim}
              actual.male
predicted.male FALSE  TRUE
         FALSE 19466  5494
         TRUE   4623 30243
\end{verbatim}
\begin{alltt}
\hlkwd{prop.table}\hlstd{(}\hlkwd{table}\hlstd{(predicted.male, actual.male),} \hlnum{2}\hlstd{)}
\end{alltt}
\begin{verbatim}
              actual.male
predicted.male FALSE TRUE
         FALSE  0.81 0.15
         TRUE   0.19 0.85
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \section{Checking assumptions}

    \begin{frame}{Checking assumptions}
      \begin{itemize}
        \item Independence
        \item Linearity
        \item Normality of residuals \redx
        \item Homoscedasticity / Equal variance \redx
      \end{itemize}
      With logistic regression, we don't need to check the last two assumptions (since $Y$ is binary).
    \end{frame}

    \begin{frame}{Checking assumptions: Independence}
      Like with linear regression, we check independence by thinking about the data conceptually: are the predictions the model makes likely to be independent from each other?
      \bigskip\pause

      \greencheckmark \alert{Yes!} Each case is a completely different person whose heights and genders are unrelated.
    \end{frame}

    \begin{frame}{Checking assumptions: Linearity}
      Look at the logistic regression model:
      \[
        \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X + \epsilon
      \]
      We need an approximately linear relationship between the \alert{log odds of success} and $X$, or, equivalently, a linear relationship between the log odds of success and what is predicted from our linear model on the right side of the equation.
    \end{frame}

    \begin{frame}[fragile]{Checking assumptions: Linearity}
      

      To do this, we segment the predicted log odds into groups by deciles (bottom 10\%, next 10\%, up until the highest 10\%):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{quantile}\hlstd{(}\hlkwd{predict}\hlstd{(model),} \hlkwc{probs}\hlstd{=}\hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0.1}\hlstd{))}
\end{alltt}
\begin{verbatim}
   0%   10%   20%   30%   40%   50%   60%   70% 
-8.04 -2.75 -1.42 -0.76 -0.10  0.56  1.88  2.55 
  80%   90%  100% 
 3.21  3.87  8.50 
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}{Checking assumptions: linearity}
      Then we'll calculate the empirical log odds within each group:

      \begin{center}
        \begin{tabular}{lllll}
          Predicted log odds & \# males & Total & $p=P(\text{male})$ & Log odds \\
          \hline
          $[ -8.04, -2.75 ]$ & $256$ &  $7182$ & $0.04$ & $-3.3$ \\
          $[ -2.75, -1.42 ]$ & $1090$ &  $7659$ & $0.14$ & $-1.8$ \\
          $[ -1.42, -0.76 ]$ & $1579$ &  $4759$ & $0.33$ & $-0.7$ \\
          $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
          $[ 3.87, 8.5 ]$ & $5168$ &  $5208$ & $0.99$ & $4.85$ \\

        \end{tabular}
      \end{center}

      Then we'll plot the empirical log odds against the mean of each decile; we'd like to see approximately the line $y=x$; this is called an \alert{empirical logit plot}.
    \end{frame}

    \begin{frame}[fragile]{Checking assumptions: Linearity}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{empirical.logit.plot}\hlstd{(model)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-8-1} 

\end{knitrout}
      \pause\vspace{-0.5cm}
      \greencheckmark \alert{Yes!} This is approximately along the line $y=x$.
    \end{frame}

    \section{Logistic regression with 2+ predictors}

    \begin{frame}{Adding another predictor}
      \begin{itemize}
        \item Just like with a linear regression model, we can add additional predictors to the model.
        \item Our interpretation of the coefficients in multiple logistic regression is similar to multiple linear regression, in the sense that each coefficient represents the predicted effect of one $X$ on $Y$, holding the other $X$ variables constant.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Adding another predictor}
      Let's add sexual orientation as a second predictor of gender, in addition to height:
      
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model2} \hlkwb{<-} \hlkwd{glm}\hlstd{(male} \hlopt{~} \hlstd{height} \hlopt{+} \hlstd{orientation,}
  \hlkwc{data}\hlstd{=my.profiles,} \hlkwc{family}\hlstd{=binomial)}
\end{alltt}
\end{kframe}
\end{knitrout}
      The \texttt{orientation} variable has three categories:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{table}\hlstd{(my.profiles}\hlopt{$}\hlstd{orientation)}
\end{alltt}
\begin{verbatim}

bisexual      gay straight 
    2763     5568    51495 
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]
      \fontvsm\vspace{-0.3cm}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{verbatim}

Call:
glm(formula = male ~ height + orientation, family = binomial, 
    data = my.profiles)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-3.620  -0.481   0.198   0.530   4.022  

Coefficients:
                     Estimate Std. Error z value Pr(>|z|)    
(Intercept)         -46.08076    0.37167  -124.0   <2e-16 ***
height                0.66535    0.00537   124.0   <2e-16 ***
orientationgay        2.09556    0.07209    29.1   <2e-16 ***
orientationstraight   1.39972    0.06068    23.1   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 80654  on 59825  degrees of freedom
Residual deviance: 43722  on 59822  degrees of freedom
AIC: 43730

Number of Fisher Scoring iterations: 6
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}{Interpreting coefficients}
      Our prediction equation is:
      
      \[
        \log\left(\frac{p}{1-p}\right) =
          -46.08 +
          0.67\cdot\text{height} +
          2.1\cdot\text{gay} +
          1.4\cdot\text{straight}.
      \]
      This means that:
      \begin{itemize}[<+->]
        \item Our predicted log odds of being male for someone who is bisexual and has a height of 0" is $-46.08$ (the intercept).
        \item Among people with the same sexual orientation, each additional inch of height corresponds to an increase in 95\% in predicted odds of being male (i.e., multiplied by $e^{0.67} = 1.95$).
      \end{itemize}
    \end{frame}

    \begin{frame}{Interpreting coefficients}
      \[
        \log\left(\frac{p}{1-p}\right) =
          -46.08 +
          0.67\cdot\text{height} +
          2.1\cdot\text{gay} +
          1.4\cdot\text{straight}.
      \]
      \begin{itemize}[<+->]
        \item Among people of the same height, being gay increases the predicted odds of being male by 713\% (i.e., multiplied by $e^{2.1} = 8.13$) compared to being bisexual.
        \item Among people of the same height, being straight increases the predicted odds of being male by 305\% (i.e., multiplied by $e^{1.4} = 4.05$) compared to being bisexual.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Understanding what's going on}
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{crosstabs} \hlkwb{<-} \hlkwd{table}\hlstd{(my.profiles}\hlopt{$}\hlstd{sex, my.profiles}\hlopt{$}\hlstd{orientation)}
\hlstd{crosstabs}
\end{alltt}
\begin{verbatim}
   
    bisexual   gay straight
  f     1994  1586    20509
  m      769  3982    30986
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}[fragile]
      \fontsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{barplot}\hlstd{(}\hlkwd{prop.table}\hlstd{(crosstabs,} \hlnum{2}\hlstd{),} \hlkwc{col}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"pink"}\hlstd{,} \hlstr{"lightblue"}\hlstd{),}
  \hlkwc{legend}\hlstd{=T)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-15-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{Converting back to probabilities}
      Because there is a nonlinear relationship between probability and odds, a particular percentage increase in odds does not correspond to a fixed change in probability. But it can be useful sometimes to compute some exemplar predicted probabilities to get a sense of the relationships:

      

      \begin{center}
        \begin{tabular}{r|llll}
          & \multicolumn{4}{c}{Height} \\
          & 60" & 64" & 68" & 72" \\
          \hline
          bisexual
            & 0.002
            & 0.029
            & 0.302
            & 0.861
            \\
          gay
            & 0.017
            & 0.197
            & 0.779
            & 0.981
            \\
          straight
            & 0.008
            & 0.109
            & 0.637
            & 0.962
            \\
        \end{tabular}
      \end{center}
    \end{frame}

    \begin{frame}
      We can also visualize this by plotting the three curves for straight (yellow), gay (green), and bisexual (blue) OkCupid users:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-17-1} 

\end{knitrout}
      Where will the curve for bisexual OkCupid users be?
    \end{frame}

    \begin{frame}
      We can also visualize this by plotting the three curves for straight (yellow), gay (green), and bisexual (blue) OkCupid users:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-18-1} 

\end{knitrout}
    \end{frame}

    \section{Interactions in logistic regression}

    \begin{frame}{What would interactions do?}
      \begin{itemize}
        \item In linear regression, an interaction between two predictors $X_1$ and $X_2$ means that the \alert{slope} of $X_1$ will depend on the \alert{value} of $X_2$.
        \item In other words, there will be differently-sloped regression lines predicting $Y$ from $X_1$ depending on what the value of $X_2$ is.
      \end{itemize}
    \end{frame}

    \begin{frame}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-19-1} 

\end{knitrout}
    \end{frame}

    \begin{frame}{What would interactions do?}
      \begin{itemize}
        \item We can add interactions to logistic regression and the interpretation is the same: the effect of $X_1$ on the \alert{probability of being male} depends on the \alert{value} of $X_2$.
        \item Let's try this out with $X_1=\text{height}$ and $X_2=\text{orientation}$.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]
      \fontvsm
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{int.model} \hlkwb{<-} \hlkwd{glm}\hlstd{(male} \hlopt{~} \hlstd{height} \hlopt{*} \hlstd{orientation,} \hlkwc{data}\hlstd{=my.profiles,} \hlkwc{family}\hlstd{=binomial)}
\hlkwd{summary}\hlstd{(int.model)}
\end{alltt}
\begin{verbatim}

Call:
glm(formula = male ~ height * orientation, family = binomial, 
    data = my.profiles)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-3.655  -0.470   0.194   0.521   4.064  

Coefficients:
                           Estimate Std. Error z value Pr(>|z|)    
(Intercept)                -35.3027     1.4050  -25.13  < 2e-16 ***
height                       0.5076     0.0206   24.67  < 2e-16 ***
orientationgay              -6.2727     1.8365   -3.42  0.00064 ***
orientationstraight        -10.2887     1.4596   -7.05  1.8e-12 ***
height:orientationgay        0.1218     0.0271    4.49  7.1e-06 ***
height:orientationstraight   0.1712     0.0214    8.01  1.2e-15 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 80654  on 59825  degrees of freedom
Residual deviance: 43663  on 59820  degrees of freedom
AIC: 43675

Number of Fisher Scoring iterations: 6
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{frame}

    \begin{frame}
      The interaction model is:

      \begin{align*}
        \log\left(\frac{p}{1-p}\right) &=
          -35.3 +
          0.51\cdot\text{height}
          - 6.27\cdot\text{gay}
          - 10.29\cdot\text{straight} \\ & \qquad+
          0.12\cdot\text{height}\cdot\text{gay} +
          0.17\cdot\text{height}\cdot\text{straight}.
      \end{align*}
    \end{frame}

    \begin{frame}
      Let's graph the equation for gay (green), straight (yellow), and bisexual (blue) users:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.137, 0.137, 0.137}\color{fgcolor}
\includegraphics[width=\maxwidth]{/tmp/figures/unnamed-chunk-22-1} 

\end{knitrout}
    \end{frame}

    \section{Other applications of logistic regression}

    \begin{frame}{What else can we use logistic regression for?}
      \begin{itemize}
        \item \textbf{Finance:} Predicting which customers are most likely to default on a loan
        \item \textbf{Advertising:} Predicting when a customer will respond positively to an advertising campaign
        \item \textbf{Marketing:} Predicting when a customer will purchase a product or sign up for a service
      \end{itemize}
    \end{frame}
  \end{darkframes}
\end{document}
