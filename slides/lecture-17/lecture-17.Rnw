\documentclass{beamer}
\usepackage{../371g-slides}
\usepackage{dcolumn}
\title{Building Models}
\subtitle{Lecture 17}
\author{STA 371G}

\begin{document}

<<setup, include=F, cache=F>>=
  library(knitr)
  opts_knit$set(global.par=T)
  opts_chunk$set(dev="cairo_pdf", dev.args=list(bg="transparent"), external=F, fig.path='/tmp/figures/', comment=NA, fig.width=4.5, fig.height=3)
  library(extrafont)
  loadfonts()
  knit_theme$set('camo')
  @
  <<include=F, cache=F>>=
  par(fg='#fefefe', col.axis='#fefefe', col.lab='#fefefe', col.main="#fefefe", mar=c(5.1, 4.1, 1.1, 2.1), family='Lato')
  library(car)
  manager <- read.csv("../../data/manager.csv", na.strings="")
  counties <- read.csv("../../data/counties.csv", na.strings="")
  counties$PhysiciansPer10000 <-
    counties$Physicians / counties$Population * 10000
  my.counties <- subset(counties, Population > 10000)
  mclean <- subset(manager, Salary > 0 & Salary < 200 & YearsExp < 99)
  @

  \frame{\maketitle}

  % Show outline at beginning of each section
  \AtBeginSection[]{
    \begin{frame}<beamer>
      \tableofcontents[currentsection]
    \end{frame}
  }

  %%%%%%% Slides start here %%%%%%%

  \begin{darkframes}
    \begin{frame}{My review of the midsemester survey}
      \begin{itemize}[<+->]
        \item Thanks for the feedback---appreciate it!
        \item In class: increasing opportunities for practice with R and with concepts in class
        \item Getting help outside of class: R info pages, LC solutions, meeting outside of office hours
      \end{itemize}
    \end{frame}

    \section{Multicollinearity}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      Multicollinearity exists whenever 2+ predictors in a regression model are moderately or highly correlated. \pause
      \begin{itemize}[<+->]
        \item If two predictors $X_1$ and $X_2$ are highly correlated, it is hard to estimate the effect of changing $X_1$ while keeping $X_2$ constant.
        \item This means we will have large standard errors, and large p-values, for $X_1$ and/or $X_2$.
        \item This \alert{does not} mean there isn't a relationship between $X_1$ and $Y$, or $X_2$ and $Y$ -- it just means we can't pin down that relationship, because of the correlation!
      \end{itemize}
      \pause
      Correlation between the response and the predictors is good, but correlation between the predictors is not!
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      We want to avoid multicollinearity in our models! \pause
      \begin{itemize}[<+->]
        \item Any conclusions based on the p-values, coefficients, and confidence intervals of the highly correlated variables will be unreliable.
        \item These statistics will not be stable: adding new data or predictors to the model could drastically change them.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]%{Exploring the data: Multicollinearity}
      \fontsize{9}{9}\selectfont
      <<fig.height=3>>=
      pairs(~ MngrRating + YearsExp + YrsSinceGrad, data=mclean)
      @
    \end{frame}

    \begin{frame}[fragile]%{Exploring the data: Multicollinearity}
      \fontsize{8}{8}\selectfont
      <<>>=
      model <- lm(Salary ~ MngrRating + YearsExp + YrsSinceGrad,
                 data=mclean)
      summary(model)
      @
      \note{Point out the high standard errors}
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      One way to see if two variables are collinear is to check the correlation between the two:
      <<>>=
      cor(mclean$YearsExp, mclean$YrsSinceGrad)
      @
      \pause
      Any correlation $\geq 0.95$ is definitely a problem, but smaller correlations could be problematic too.
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      A better way to check multicollinearity is using Variance Inflation Factors (VIF).

      \begin{itemize}[<+->]
        \item The VIF is
          \[
            \text{VIF}(\beta_j) = \frac{1}{1 - R_j^2},
          \]
          where $R_j^2$ is the $R^2$ in a regression predicting $X$ variable $j$ from the other $X$ variables.
        \item $\text{VIF}(\beta_j)=0$ when $R_j^2=0$; i.e., the $j$th predictor variable is completely independent from the others.
        \item $\text{VIF}(\beta_j)$ increases as $R_j^2$ does, and is $\infty$ when there is perfect multicollinearity; i.e., when $X_j$ is perfectly predictable from the other $X$ variables.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Exploring the data: Multicollinearity}
      <<>>=
      library(car)
      vif(model)
      @
      Predictors with $\text{VIF} > 5$ indicate multicollinearity.
      \pause\bigskip

      \alert{Remember:} Multicollinearity could exist between more than two predictors (this is why there are only $n-1$ dummy variables for a categorical variable with $n$ values).
    \end{frame}

    \begin{frame}
      \fullpagepicture{debbie-downer}
    \end{frame}

    \begin{frame}{Dealing with multicollinearity}
      There are two general strategies for dealing with multicollinearity:
      \begin{itemize}
        \item Drop a variable with a high VIF factor. (Just like we drop one of the dummy variables when putting a categorical variable in the model!)
        \item Combine the variables that correlate into a composite variable.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]%{Exploring the data: Multicollinearity}
      \fontsize{9}{9}\selectfont
      <<>>=
      model2 <- lm(Salary ~ MngrRating + YearsExp, data=mclean)
      summary(model2)
      @
    \end{frame}

    \section{Selecting the best model}
    \begin{frame}
      \fullpagepicture{doc-shortage}
    \end{frame}

    \begin{frame}[fragile]{Potential predictor variables}
      \begin{itemize}
        \item \textbf{LandArea}:       Area in square miles
        \item \textbf{PctRural}:       Percentage rural land
        \item \textbf{MedianIncome}:   Median household income
        \item \textbf{Population}:     Population
        \item \textbf{PctUnder18}:     Percent children
        \item \textbf{PctOver65}:      Percent seniors
        \item \textbf{PctPoverty}:     Percent below the poverty line
        \item \textbf{PctUninsured}:   Percent without health insurance
        \item \textbf{PctSomeCollege}: Percent with some higher education
        \item \textbf{PctUnemployed}:  Percent unemployed
      \end{itemize}
    \end{frame}

    \begin{frame}{Parsimony}
      \begin{itemize}[<+->]
        \item We want a model that has a high $R^2$ and a low $s_e$, because then the predictors are doing a good job of explaining $Y$---and our predictions will be more accurate.
        \item We also want a model that is simple, so it's easy to explain to a non-expert.
        \item The ideal model is \alert{parsimonious}: a good trade-off between simplicity (as few variables as possible) and a high $R^2$.
      \end{itemize}
    \end{frame}

    \begin{frame}{General strategy}
      \begin{enumerate}[<+->]
        \item Use one or more procedures to generate candidate models: possible models that are worth considering.
        \item Select the candidate model with a reasonable tradeoff simplicity and predictive power (high $R^2$).
        \item Check assumptions; apply transformations and other fixes if needed to the final model. If the problems are unfixable, select a different candidate model.
      \end{enumerate}
    \end{frame}

    \begin{frame}{Backward stepwise regression}
      \begin{enumerate}
        \item Start with a ``full'' model containing all of the predictors.
        \item Remove the least significant (highest $p$-value / smallest $t$-statistic) predictor.
        \item Re-run the model with that predictor removed.
        \item Repeat steps 2-3 until all predictors are significant.
      \end{enumerate}
    \end{frame}

    \begin{frame}{Forward stepwise regression}
      \begin{enumerate}
        \item Start with a ``null'' model containing none of the predictors.
        \item Try adding each predictor, one at a time, and pick the one that ends up being the most significant (lowest $p$-value / highest $t$-statistic) predictor.
        \item Re-run the model with that predictor added.
        \item Repeat steps 2-3 until no more significant predictors can be added.
      \end{enumerate}
    \end{frame}

    \begin{frame}{Other stepwise regression possibilities}
      \begin{itemize}
        \item Add (or remove) variables one at a time based on the change in $R^2$, Adjusted $R^2$, or another model fit criterion when that variable is added (or removed).
        \item Run the stepwise regression in both directions, allowing addition or removal of a variable at each step.
        \item R's \texttt{step} function incorporates both of these methods.
      \end{itemize}
    \end{frame}

    \begin{frame}{The problem with stepwise regression}
      Stepwise regression will not necessarily give you the best model; by only adding or removing one variable at a time, you can get locked into a particular ``path'' that means you may never consider better models.
    \end{frame}

    \begin{frame}{Best subsets regression}
      \begin{itemize}
        \item Computers are fast! Just let R try out all of the $2^k-1$ possible models for you.
        \item R will present you the model with the best Adjusted $R^2$ for each possible number of predictors.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Best-subsets regression}
      \fontsm
       <<fig.keep="none">>=
       library(leaps)
       plot(regsubsets(PhysiciansPer10000 ~ LandArea + PctRural
                        + MedianIncome + Population + PctUnder18
                        + PctOver65 + PctPoverty + PctUninsured
                        + PctSomeCollege + PctUnemployed,
                        data=my.counties), scale="adjr2")
       @
    \end{frame}

    \begin{frame}
      \vspace{-1in}
      <<echo=F, fig.height=4.5>>=
      plot(regsubsets(PhysiciansPer10000 ~ LandArea + PctRural
                       + MedianIncome + Population + PctUnder18
                       + PctOver65 + PctPoverty + PctUninsured
                       + PctSomeCollege + PctUnemployed,
                       data=my.counties), scale="adjr2", col='orange')
      @
    \end{frame}

    \begin{frame}
      \begin{itemize}
        \item Best-subsets regression presents us with a candidate model for each possible number of predictors.
        \item The label on the $y$-axis show the Adjusted $R^2$ value for the model corresponding to the filled-in squares for that row.
      \end{itemize}
    \end{frame}

    \begin{frame}{Putting things together}
      \begin{itemize}[<+->]
        \item Look at multiple statistics. They generally say similar things.
        \item Find the \textbf{parsimonious} middle ground between an underspecified model and extraneous variables.
        \item Fine-tune the model to ensure the model meets assumptions and captures key relationships: you may need to transform predictors and/or add interactions.
        \item Think about logical reasons why certain predictors might be useful; don't just focus on $p$-values.
      \end{itemize}
    \end{frame}

    \begin{frame}[fragile]{Check assumptions of the best model}
      \fontsm
      <<echo=F>>=
      par(mfrow=c(2,2), mar=c(2,2,2,2)) # change the panel layout to 2x2
      @
      <<fig.height=2.5>>=
      candidate <- lm(PhysiciansPer10000 ~ PctRural + PctOver65
                        + PctSomeCollege, data=my.counties)
      plot(candidate)
      @
      <<echo=F>>=
      old.r2 <- summary(candidate)$r.squared
      par(mfrow=c(1,1), mar=c(5.1, 4.1, 4.1, 2.1)) # change the panel layout back
      @
    \end{frame}

    \begin{frame}{Be careful of getting too crazy!}
      \begin{itemize}[<+->]
        \item A general guideline is that you should not even consider more than one variable for every 10 to 15 cases in your dataset.
        \item Think about the meaning of the variables in your model. Be careful if the model looks too good to be true.
        \item Do not just use a mechanical process for model selection and call it a day; you need to use your judgement and select a parsimonious model.
        \item Don't forget to check the model assumptions for your final model!
      \end{itemize}
    \end{frame}

  \end{darkframes}

\end{document}
